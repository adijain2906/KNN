{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After your mail I tried out that sigmoid method, but was not able to implement it so I figured out that reducing the learning rate to a lower value such as 0.001, might help in not generating the overflow error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating sigmoid of a number\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) #np.where(x<0, np.exp(x) / (1 + np.exp(x)), 1 / (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Activate(X,Weights,Weightsfinal):\n",
    "    H = np.dot(Weights,X.T)                                               \n",
    "    H = np.c_[H.T, np.ones(len(X))]      #Computing the dot product of Inputs and Weights\n",
    "    Z = np.dot(H, Weightsfinal)          #Computing the dot product of H and FinalWeights\n",
    "    Activation = sigmoid(Z)\n",
    "    return Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the classes based on the value of Activation\n",
    "def predict(x):\n",
    "    return np.where(x>0.5,0.9,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for training of MLP\n",
    "def MLP(X,y):\n",
    "    iterations = 15000\n",
    "    alpha = 0.001\n",
    "    n_layer1 = 3\n",
    "    inputs = len(X[0])\n",
    "    Weights = 0.01 * np.random.rand(n_layer1, inputs)   #Weights for Inputs\n",
    "    Weightsfinal = 0.01 * np.random.rand(n_layer1+1, 1) #Weights for Hidden Layer\n",
    "                \n",
    "    for t in range(iterations):\n",
    "        Activation = Activate(X,Weights, Weightsfinal)\n",
    "        dB = (Activation.T - y).T\n",
    "        \n",
    "        H = np.dot(Weights,X.T)                                               \n",
    "        H = np.c_[H.T, np.ones(len(X))] \n",
    "        dW_ = np.dot(H.T, dB)\n",
    "        Weightsfinal -= alpha*dW_            #Changing the FinalWeights\n",
    "        dw = (np.dot(np.dot(X.T,dB),Weightsfinal[:-1].T)).T \n",
    "        Weights -= alpha*dw                  #Changing the Weights of Inputs\n",
    "        yhat = predict(Activation)           #Predicting using activation\n",
    "        \n",
    "        if t%1000 == 0:\n",
    "            print(\"Iteration:\",t, \"- Accuracy:\",np.sum(train_y.T==yhat)/len(X) * 100, \"%\")\n",
    "    \n",
    "    print(\"\\nFinal Accuracy:\",np.sum(y.T==yhat)/len(X) * 100, \"%\")\n",
    "    print(\"Predicted:\", yhat.T)\n",
    "    print(\"Actual:   \",train_y)\n",
    "    \n",
    "    return Weights, Weightsfinal\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 - Accuracy: 50.0 %\n",
      "Iteration: 1000 - Accuracy: 50.0 %\n",
      "Iteration: 2000 - Accuracy: 42.857142857142854 %\n",
      "Iteration: 3000 - Accuracy: 50.0 %\n",
      "Iteration: 4000 - Accuracy: 50.0 %\n",
      "Iteration: 5000 - Accuracy: 50.0 %\n",
      "Iteration: 6000 - Accuracy: 57.14285714285714 %\n",
      "Iteration: 7000 - Accuracy: 57.14285714285714 %\n",
      "Iteration: 8000 - Accuracy: 64.28571428571429 %\n",
      "Iteration: 9000 - Accuracy: 64.28571428571429 %\n",
      "Iteration: 10000 - Accuracy: 71.42857142857143 %\n",
      "Iteration: 11000 - Accuracy: 71.42857142857143 %\n",
      "Iteration: 12000 - Accuracy: 71.42857142857143 %\n",
      "Iteration: 13000 - Accuracy: 71.42857142857143 %\n",
      "Iteration: 14000 - Accuracy: 71.42857142857143 %\n",
      "\n",
      "Final Accuracy: 71.42857142857143 %\n",
      "Predicted: [[0.9 0.9 0.9 0.9 0.1 0.1 0.9 0.9 0.1 0.1 0.1 0.1 0.9 0.1]]\n",
      "Actual:    [[0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.1 0.1 0.1 0.1 0.1 0.1 0.1]]\n"
     ]
    }
   ],
   "source": [
    "data = [[9, 9, 0.9], [10, 10, 0.9], [9, 10, 0.9], [10, 9, 0.9], [11, 11,0.9], [10, 11, 0.9], [11, 10,0.9],\n",
    "        [12,9, 0.1], [9,  12, 0.1], [14, 10, 0.1], [9, 13, 0.1], [10, 13, 0.1], [11, 8,  0.1], [8, 12, 0.1]]\n",
    "\n",
    "data = np.asarray(data)\n",
    "\n",
    "X = np.asarray([row[:-1] for row in data])  #Getting all the inputs\n",
    "X = np.c_[X,np.ones(len(X))]   #Extra 1s are added in the data to make it is for the dot product with weights and bias\n",
    "y = np.asarray([[row[-1] for row in data]]) #Getting all the classes\n",
    "\n",
    "Weights, Weightsfinal = MLP(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
